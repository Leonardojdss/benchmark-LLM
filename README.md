# Benchmark LLM - Comparativo de Modelos de Linguagem

Este projeto Ã© um laboratÃ³rio prÃ¡tico para benchmarking de modelos de linguagem de grande porte (LLMs), incluindo GPT-OSS, GPT-4.1, o1-mini, o3-mini, Gemini, Claude e outros. O objetivo Ã© avaliar e comparar latÃªncia de resposta e resoluÃ§Ã£o de questÃµes complexas de vestibulares (FUVEST, ITA).

## ğŸ“Š CaracterÃ­sticas Avaliadas

- **LatÃªncia de Resposta**: Tempo de resposta para completaÃ§Ãµes de texto
- **PrecisÃ£o**: Capacidade de resolver questÃµes complexas de matemÃ¡tica, fÃ­sica e quÃ­mica, provas de universidades

## ğŸ¤– Modelos Testados

### OpenAI
- `gpt-4.1-mini`
- `gpt-4o`
- `o1-mini`
- `o3-mini`

### Google/Gemini
- `gemini-2.5-flash`
- `gemini-2.5-flash-lite`
- `gemma-3-27b-it`

### Anthropic
- `claude-opus-4-1-20250805`
- `claude-sonnet-4-20250514`

### OpenAI via Groq
- `openai/gpt-oss-120b`
- `openai/gpt-oss-20b`

## ğŸ›  ConfiguraÃ§Ã£o do Ambiente

### PrÃ©-requisitos
- Python 3.8+

### InstalaÃ§Ã£o

1. Clone o repositÃ³rio:
```bash
git clone https://github.com/Leonardojdss/benchmark-LLM.git
cd benchmark-LLM
```

2. Crie e ative um ambiente virtual:
```bash
python -m venv env
source env/bin/activate  # No Windows: env\Scripts\activate
```

3. Instale as dependÃªncias:
```bash
pip install -r requirements.txt
```

### ConfiguraÃ§Ã£o das APIs

Crie um arquivo `.env` na raiz do projeto com as seguintes variÃ¡veis:

```env
# APIs obrigatÃ³rias
OPENAI_API_KEY=sua_chave_openai
GEMINI_API_KEY=sua_chave_gemini
ANTHROPIC_API_KEY=sua_chave_anthropic
GROG_API_KEY=sua_chave_groq

# ConfiguraÃ§Ãµes adicionais
OPENAI_API_VERSION=2024-05-01-preview
```

### Como Obter as Chaves de API

- **OpenAI**: [Platform OpenAI](https://platform.openai.com/api-keys)
- **Gemini**: [Google AI Studio](https://aistudio.google.com/app/apikey)
- **Anthropic**: [Anthropic Console](https://console.anthropic.com/)
- **Groq**: [Groq Console](https://console.groq.com/keys)

## ğŸ“ Estrutura do Projeto

```
benchmark-LLM/
â”œâ”€â”€ benchmark_LLM.ipynb          # Notebook principal com todos os testes
â”œâ”€â”€ README.md                    # DocumentaÃ§Ã£o do projeto
â”œâ”€â”€ requirements.txt             # DependÃªncias Python
â”œâ”€â”€ .env                         # VariÃ¡veis de ambiente (criar)
â”œâ”€â”€ dados/                       # Imagens das questÃµes de teste
â”‚   â”œâ”€â”€ question_1_fuvest_quimica_c.png
â”‚   â”œâ”€â”€ question_1_fuvest_fisica_c.png
â”‚   â”œâ”€â”€ question_1_fuvest_mat_c.png
â”‚   â”œâ”€â”€ question_2_fuvest_fisica_c.png
â”‚   â”œâ”€â”€ question_2_fuvest_quimica_a.png
â”‚   â”œâ”€â”€ question_2_ita_mat_c.png
â”‚   â””â”€â”€ question_3_fuvest_mat_a.png
â””â”€â”€ env/                         # Ambiente virtual Python
```

## ğŸš€ Como Usar

### Executando o Notebook

1. Inicie o Jupyter Notebook:
```bash
jupyter notebook
```

2. Abra o arquivo `benchmark_LLM.ipynb`

3. Execute as cÃ©lulas sequencialmente para:
   - Testar latÃªncia de resposta
   - Avaliar resoluÃ§Ã£o de questÃµes complexas
   - Gerar grÃ¡ficos comparativos

### Principais Funcionalidades

#### 1. Teste de LatÃªncia
```python
# Exemplo de uso da funÃ§Ã£o de teste de latÃªncia
message = "Create a new text for me with theme about the future of technology, content 200 characters"
execution = gpt_completation_time_response("gpt-4.1-mini", message)
```

#### 2. ResoluÃ§Ã£o de QuestÃµes
```python
# Exemplo de uso da funÃ§Ã£o universal de completaÃ§Ã£o
response = completation("gpt-4.1-mini", "Resolva esta questÃ£o de matemÃ¡tica...", "text")
```
